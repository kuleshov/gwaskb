{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenotype extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demo the first module of the GWASdb system, which extracts the phenotypes that are studied in each paper.\n",
    "\n",
    "Before starting, make sure you have downloaded all the datasets: the phenotype ontologies, the GWAS Catalog database, and the open-access GWAS papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by configuring Jupyter and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "\n",
    "# set the paths to snorkel and gwasdb\n",
    "sys.path.append('../snorkel-tables')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/crawler')\n",
    "\n",
    "# set up the directory with the input papers\n",
    "abstract_dir = '../data/db/papers'\n",
    "\n",
    "# set up matplotlib\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "\n",
    "# create a Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system will read PubMed papers that have been previously identified as GWAS-related. We load this corpus below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from extractor.parser import GWASXMLAbstractParser\n",
    "\n",
    "xml_parser = GWASXMLAbstractParser(\n",
    "    path=abstract_dir,\n",
    "    doc='./*',\n",
    "    title='.//front//article-title//text()',\n",
    "    abstract='.//abstract//p//text()',\n",
    "    par1='.//body/p[1]//text()',\n",
    "    id='.//article-id[@pub-id-type=\"pmid\"]/text()',\n",
    "    keep_xml_tree=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GWASXMLAbstractParser` is a custom parser that we wrote. For each paper, it extracts the title and either the abstract of the first paragraph (if there is no abstract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 2.94 s, total: 23.9 s\n",
      "Wall time: 3min 16s\n",
      "Loaded corpus of 589 documents\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import SentenceParser\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Corpus\n",
    "\n",
    "# this splits documents into sentences and parses each sentence with Stanford CoreNLP\n",
    "sent_parser = SentenceParser(timeout=600000)\n",
    "\n",
    "try:\n",
    "    corpus = session.query(Corpus).filter(Corpus.name == 'GWAS Corpus').one()\n",
    "except:\n",
    "    cp = CorpusParser(xml_parser, sent_parser)\n",
    "    %time corpus = cp.parse_corpus(name='GWAS Corpus', session=session)\n",
    "    session.add(corpus)\n",
    "    session.commit()\n",
    "\n",
    "print 'Loaded corpus of %d documents' % len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage is to generate a large set of candidate phenotypes, which may or may not be correct. After that, we will train classifiers to predict which ones are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load our phenotype ontologies, which will be used to generate candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from db.kb import KnowledgeBase\n",
    "from extractor.util import make_ngrams\n",
    "\n",
    "# collect phenotype list\n",
    "kb = KnowledgeBase()\n",
    "\n",
    "# efo phenotypes\n",
    "efo_phenotype_list0 = kb.get_phenotype_candidates(source='efo-matching', peek=False) # TODO: remove peaking\n",
    "efo_phenotype_list = list(make_ngrams(efo_phenotype_list0))\n",
    "# snomed keywords\n",
    "snomed_phenotype_list = kb.get_phenotype_candidates(source='snomed')\n",
    "# mesh diseases\n",
    "mesh_phenotype_list0 = kb.get_phenotype_candidates(source='mesh')\n",
    "mesh_phenotype_list = list(make_ngrams(mesh_phenotype_list0))\n",
    "# mesh chemicals\n",
    "chem_phenotype_list = kb.get_phenotype_candidates(source='chemical')\n",
    "# regex matches\n",
    "rgx = u'[A-Za-z\\u2013-]+ (disease|trait|phenotype|outcome|response|quantitative trait|measurement|response|side effects)s?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define matchers and an extractor that generate candaites based on these ontologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "from snorkel.matchers import DictionaryMatch, Union, RegexMatchSpan\n",
    "from extractor.matcher import PhenotypeMatcher\n",
    "from extractor.util import change_name\n",
    "\n",
    "# Define a candidate space\n",
    "ngrams = Ngrams(n_max=7)\n",
    "\n",
    "# Define a matcher for each ontology\n",
    "efo_phen_matcher = PhenotypeMatcher(d=efo_phenotype_list, ignore_case=True, mod_fn=change_name)\n",
    "snom_phen_matcher = PhenotypeMatcher(d=snomed_phenotype_list, ignore_case=True, mod_fn=change_name)\n",
    "mesh_phen_matcher = PhenotypeMatcher(d=mesh_phenotype_list, ignore_case=True, mod_fn=change_name)\n",
    "chem_phen_matcher = DictionaryMatch(d=chem_phenotype_list, longest_match_only=True, ignore_case=True)\n",
    "regex_phen_matcher = RegexMatchSpan(rgx=rgx)\n",
    "\n",
    "# The phenotype matcher is the union of these\n",
    "phen_matcher = Union(efo_phen_matcher, snom_phen_matcher, mesh_phen_matcher, chem_phen_matcher, regex_phen_matcher)\n",
    "\n",
    "# Define the extractor\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Phenotype = candidate_subclass('SnorkelPhenotype', ['phenotype'])\n",
    "phen_extractor = CandidateExtractor(Phenotype, ngrams, phen_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we extract the candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6863 sentences loaded\n",
      "[========================================] 100%\n",
      "CPU times: user 14min 3s, sys: 5.31 s, total: 14min 8s\n",
      "Wall time: 14min 21s\n",
      "71476 candidates extracted\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    phen_c = session.query(CandidateSet).filter(CandidateSet.name == 'Phenotype Candidates').one()\n",
    "except:\n",
    "    sentences = [s for doc in corpus for s in doc.sentences]\n",
    "    print '%d sentences loaded' % len(sentences)\n",
    "    %time phen_c = phen_extractor.extract(sentences, 'Phenotype Candidates', session)\n",
    "    session.add(phen_c)\n",
    "    session.commit()\n",
    "\n",
    "print '%d candidates extracted' % len(phen_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to remove nested candidates as well as obviously wrong candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64008 candidates dropped, now we have 7468\n"
     ]
    }
   ],
   "source": [
    "from extractor.candidates import deduplicate, filter_cand\n",
    "\n",
    "# we filter candidates or candidates that don't occur within first 3 sentences\n",
    "# TODO: add stopwords: genome, association, population, analysis\n",
    "def filter_fn(cand, attrib='phenotype'):\n",
    "    txt = getattr(cand, attrib).get_span()\n",
    "    sent_n = getattr(cand, attrib).parent.position\n",
    "    return False if len(txt) < 5 or sent_n > 2 else True\n",
    "\n",
    "try:\n",
    "    new_phen_c = session.query(CandidateSet).filter(CandidateSet.name == 'Filtered Phenotype Candidates').one()\n",
    "except:\n",
    "    new_phen_c = CandidateSet(name='Filtered Phenotype Candidates')\n",
    "    for cand in filter_cand(deduplicate(phen_c), filter_fn=filter_fn):\n",
    "        new_phen_c.append(cand)\n",
    "    session.add(new_phen_c)\n",
    "    session.commit()\n",
    "\n",
    "print len(phen_c) - len(new_phen_c), 'candidates dropped, now we have', len(new_phen_c)\n",
    "phen_c = new_phen_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the correctness of our candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train machine learning models to identify which phenotype candidates are actually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a labeled set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split data into an (unlabeled) training set (since we will use unsupervised risk estimation to train a candidate on it), and a dev/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3734.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 3734 training and 3734 dev/testing candidates\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_c = session.query(CandidateSet).filter(CandidateSet.name == 'Phenotype Training Candidates').one()\n",
    "    devtest_c = session.query(CandidateSet).filter(CandidateSet.name == 'Phenotype Dev/Test Candidates').one()\n",
    "except:\n",
    "    # delete any previous sets with that name\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'Phenotype Training Candidates').delete()\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'Phenotype Dev/Test Candidates').delete()\n",
    "\n",
    "    frac_test = 0.5\n",
    "\n",
    "    # initialize the new sets\n",
    "    train_c = CandidateSet(name='Phenotype Training Candidates')\n",
    "    devtest_c = CandidateSet(name='Phenotype Dev/Test Candidates')\n",
    "\n",
    "    # choose a random subset for the labeled set\n",
    "    n_test = len(phen_c) * frac_test\n",
    "    test_idx = set(np.random.choice(len(phen_c), size=(n_test,), replace=False))\n",
    "\n",
    "    # add to the sets\n",
    "    for i, c in enumerate(phen_c):\n",
    "        if i in test_idx:\n",
    "            devtest_c.append(c)\n",
    "        else:\n",
    "            train_c.append(c)\n",
    "\n",
    "    # save the results\n",
    "    session.add(train_c)\n",
    "    session.add(devtest_c)\n",
    "    session.commit()\n",
    "\n",
    "print 'Initialized %d training and %d dev/testing candidates' % (len(train_c), len(devtest_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will come back to this split later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the data programming approach, we define set of labeling functions. We will learn their accuracy via unsupervised learning and use them for classifying candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to preload data form our phenotype dictionaries, and we also define common stopwords that we will try to filter out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.stem import PorterStemmer\n",
    "from db.kb import KnowledgeBase\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# load set of dictionary phenotypes\n",
    "kb = KnowledgeBase()\n",
    "phenotype_list = kb.get_phenotype_candidates() # TODO: load disease names from NCBI\n",
    "phenotype_list = [phenotype for phenotype in phenotype_list]\n",
    "phenotype_set = set(phenotype_list)\n",
    "\n",
    "# load stopwords\n",
    "with open('../data/phenotypes/snorkel/dicts/manual_stopwords.txt') as f:\n",
    "    stopwords = {line.strip() for line in f}\n",
    "stopwords.update(['analysis', 'age', 'drug', 'community', 'detect', 'activity', 'genome',\n",
    "                  'genetic', 'phenotype', 'response', 'population', 'parameter', 'diagnosis',\n",
    "                  'level', 'survival', 'maternal', 'paternal', 'clinical', 'joint', 'related',\n",
    "                  'status', 'risk', 'protein', 'association', 'signal', 'pathway', 'genotype', 'scale',\n",
    "                  'human', 'family', 'heart', 'general', 'chromosome', 'susceptibility', 'select', \n",
    "                  'medical', 'system', 'trait', 'suggest', 'confirm', 'subclinical', 'receptor', \n",
    "                  'class', 'adult', 'affecting', 'increase'])\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "stopwords.update(nltk_stopwords.words('english'))\n",
    "stopwords = {stemmer.stem(word) for word in stopwords}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a few helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phenotype(entity, stem=False):\n",
    "    phenotype = entity.get_span()\n",
    "    if stem: phenotype = stemmer.stem(phenotype)\n",
    "    return phenotype.lower()\n",
    "\n",
    "def stem_list(L):\n",
    "    return [stemmer.stem(l.lower()) for l in L]\n",
    "\n",
    "def span(c):\n",
    "    return c if isinstance(c, TemporarySpan) else c[-1]\n",
    "\n",
    "def has_stopwords(m):\n",
    "    txt = span(m).get_span()\n",
    "    txt = ''.join(ch for ch in txt if ch not in punctuation)\n",
    "    words = txt.lower().split()\n",
    "    return True if all(word in stopwords for word in words) or \\\n",
    "                  all(stemmer.stem(word) in stopwords for word in words) or \\\n",
    "                  all(change_name(word) in stopwords for word in words) else False\n",
    "        \n",
    "# candidate_by_sent = dict()\n",
    "# for c in phen_c:\n",
    "#     if has_stopwords(c): continue\n",
    "#     context_id = c[0].parent.document.name, c[0].parent.sentence.position\n",
    "#     if context_id not in candidate_by_sent:\n",
    "#         candidate_by_sent[context_id] = [999]\n",
    "#     candidate_by_sent[context_id].append(c)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the functions themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with functions that are indicative of the mention being true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "from snorkel.lf_helpers import *\n",
    "\n",
    "label_manager = LabelManager()\n",
    "\n",
    "# positive LFs\n",
    "def LF_first_sentence(m):\n",
    "    return +15 if span(m).parent.position == 0 and not has_stopwords(m) else 0\n",
    "def LF_from_regex(m):\n",
    "    if span(m).parent.position == 0 and not regex_phen_matcher._f(span(m)) and not LF_bad_words(m): return +5\n",
    "    else: return 0\n",
    "def LF_with_acronym(m):\n",
    "    post_txt = ''.join(right_text(m, attr='words', window=5))\n",
    "    return +1 if re.search(r'\\([A-Z]{2,4}\\)', post_txt) else 0\n",
    "def LF_many_words(m):\n",
    "    return +1 if len(span(m).get_span().split()) >= 3 else 0\n",
    "def LF_start_of_sentence(m):\n",
    "    return +1 if m[0].get_word_start() <= 5 and not has_stopwords(m) and not LF_no_nouns(m) else 0\n",
    "def LF_first_mention_in_sentence(m):\n",
    "    context_id = m[0].parent.document.name, m[0].parent.sentence.position\n",
    "    other_pos = [c.get_word_start() for c in candidate_by_sent[context_id]]\n",
    "    return +1 if m.get_word_start() == min(other_pos) else 0\n",
    "\n",
    "LFs_pos = [LF_first_sentence, LF_with_acronym, LF_from_regex, LF_many_words, LF_start_of_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we also define functions that are indicative of the mention being spurious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# negative LFs\n",
    "def LF_bad_words(m):\n",
    "    bad_words = ['disease', 'single', 'map', 'genetic variation', '( p <']\n",
    "    return -100 if any(span(m).get_span().lower().startswith(b) for b in bad_words) else 0\n",
    "def LF_short(m):\n",
    "    txt = span(m).get_attrib_span('words', 3)\n",
    "    return -50 if len(txt) < 5 else 0\n",
    "def LF_no_nouns(m):\n",
    "    return -10 if not any(t.startswith('NN') for t in span(m).get_attrib_tokens('pos_tags')) else 0\n",
    "def LF_not_first_sentences(m):\n",
    "    return -1 if span(m).parent.position > 1 else 0\n",
    "def LF_stopwords(m):\n",
    "    return -50 if has_stopwords(m) else 0\n",
    "\n",
    "LFs_neg = [LF_bad_words, LF_short, LF_no_nouns, LF_not_first_sentences, LF_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine both functions and use them to label our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "CPU times: user 228 ms, sys: 10.5 ms, total: 239 ms\n",
      "Wall time: 240 ms\n"
     ]
    }
   ],
   "source": [
    "LFs = LFs_pos + LFs_neg\n",
    "\n",
    "try:\n",
    "    %time L_train = label_manager.load(session, train_c, 'Phenotype LF Labels')\n",
    "except sqlalchemy.orm.exc.NoResultFound:\n",
    "    %time L_train = label_manager.create(session, train_c, 'Phenotype LF Labels', f=LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below displays some statistics about our labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conflicts</th>\n",
       "      <th>coverage</th>\n",
       "      <th>j</th>\n",
       "      <th>overlaps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_first_sentence</th>\n",
       "      <td> 0.212908</td>\n",
       "      <td>  1.992501</td>\n",
       "      <td> 0</td>\n",
       "      <td>  1.992501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_with_acronym</th>\n",
       "      <td> 0.022764</td>\n",
       "      <td>  0.035083</td>\n",
       "      <td> 1</td>\n",
       "      <td>  0.027584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_from_regex</th>\n",
       "      <td> 0.827531</td>\n",
       "      <td>  1.375201</td>\n",
       "      <td> 2</td>\n",
       "      <td>  1.375201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_many_words</th>\n",
       "      <td> 0.029727</td>\n",
       "      <td>  0.115426</td>\n",
       "      <td> 3</td>\n",
       "      <td>  0.086770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_start_of_sentence</th>\n",
       "      <td> 0.015801</td>\n",
       "      <td>  0.113283</td>\n",
       "      <td> 4</td>\n",
       "      <td>  0.069363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_bad_words</th>\n",
       "      <td> 0.562400</td>\n",
       "      <td>  1.794322</td>\n",
       "      <td> 5</td>\n",
       "      <td>  1.794322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_short</th>\n",
       "      <td> 0.000000</td>\n",
       "      <td>  0.000000</td>\n",
       "      <td> 6</td>\n",
       "      <td>  0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_no_nouns</th>\n",
       "      <td> 1.387252</td>\n",
       "      <td>  5.982860</td>\n",
       "      <td> 7</td>\n",
       "      <td>  5.982860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_not_first_sentences</th>\n",
       "      <td> 0.041510</td>\n",
       "      <td>  0.326727</td>\n",
       "      <td> 8</td>\n",
       "      <td>  0.249330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_stopwords</th>\n",
       "      <td> 8.583289</td>\n",
       "      <td> 23.018211</td>\n",
       "      <td> 9</td>\n",
       "      <td> 23.018211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        conflicts   coverage  j   overlaps\n",
       "LF_first_sentence        0.212908   1.992501  0   1.992501\n",
       "LF_with_acronym          0.022764   0.035083  1   0.027584\n",
       "LF_from_regex            0.827531   1.375201  2   1.375201\n",
       "LF_many_words            0.029727   0.115426  3   0.086770\n",
       "LF_start_of_sentence     0.015801   0.113283  4   0.069363\n",
       "LF_bad_words             0.562400   1.794322  5   1.794322\n",
       "LF_short                 0.000000   0.000000  6   0.000000\n",
       "LF_no_nouns              1.387252   5.982860  7   5.982860\n",
       "LF_not_first_sentences   0.041510   0.326727  8   0.249330\n",
       "LF_stopwords             8.583289  23.018211  9  23.018211\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have defined a set of noisy labeling functions that are indicative of each mention being true or false. Next, we use data programming to train a noise-aware generative model that can estimate the accuracies of these labeling functions from data in an unsupervised manner.\n",
    "\n",
    "The resulting model can be used to provide a learning signal for a second discriminative model (as described in the data programming paper), or it can be used to make predictions directly. Here, we will follow the latter approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t3734\n",
      "Features:\t\t\t10\n",
      "================================================================================\n",
      "Begin training for rate=0.01, mu=1e-06\n",
      "\tLearning epoch = 0\tGradient mag. = 5.702781\n",
      "\tLearning epoch = 250\tGradient mag. = 1.591952\n",
      "\tLearning epoch = 500\tGradient mag. = 0.797498\n",
      "\tLearning epoch = 750\tGradient mag. = 0.545646\n",
      "\tLearning epoch = 1000\tGradient mag. = 0.401758\n",
      "\tLearning epoch = 1250\tGradient mag. = 0.310199\n",
      "\tLearning epoch = 1500\tGradient mag. = 0.249711\n",
      "\tLearning epoch = 1750\tGradient mag. = 0.207701\n",
      "\tLearning epoch = 2000\tGradient mag. = 0.176717\n",
      "\tLearning epoch = 2250\tGradient mag. = 0.152529\n",
      "\tLearning epoch = 2500\tGradient mag. = 0.132795\n",
      "\tLearning epoch = 2750\tGradient mag. = 0.116204\n",
      "\tLearning epoch = 3000\tGradient mag. = 0.101995\n",
      "\tLearning epoch = 3250\tGradient mag. = 0.089694\n",
      "\tLearning epoch = 3500\tGradient mag. = 0.078980\n",
      "\tLearning epoch = 3750\tGradient mag. = 0.069617\n",
      "\tLearning epoch = 4000\tGradient mag. = 0.061422\n",
      "\tLearning epoch = 4250\tGradient mag. = 0.054243\n",
      "\tLearning epoch = 4500\tGradient mag. = 0.047955\n",
      "\tLearning epoch = 4750\tGradient mag. = 0.042450\n",
      "\tLearning epoch = 5000\tGradient mag. = 0.037632\n",
      "\tLearning epoch = 5250\tGradient mag. = 0.033421\n",
      "\tLearning epoch = 5500\tGradient mag. = 0.029745\n",
      "\tLearning epoch = 5750\tGradient mag. = 0.026542\n",
      "\tLearning epoch = 6000\tGradient mag. = 0.023755\n",
      "\tLearning epoch = 6250\tGradient mag. = 0.021338\n",
      "\tLearning epoch = 6500\tGradient mag. = 0.019246\n",
      "\tLearning epoch = 6750\tGradient mag. = 0.017442\n",
      "\tLearning epoch = 7000\tGradient mag. = 0.015893\n",
      "\tLearning epoch = 7250\tGradient mag. = 0.014568\n",
      "\tLearning epoch = 7500\tGradient mag. = 0.013439\n",
      "\tLearning epoch = 7750\tGradient mag. = 0.012481\n",
      "\tLearning epoch = 8000\tGradient mag. = 0.011674\n",
      "\tLearning epoch = 8250\tGradient mag. = 0.010995\n",
      "\tLearning epoch = 8500\tGradient mag. = 0.010426\n",
      "\tLearning epoch = 8750\tGradient mag. = 0.009952\n",
      "\tLearning epoch = 9000\tGradient mag. = 0.009557\n",
      "\tLearning epoch = 9250\tGradient mag. = 0.009228\n",
      "\tLearning epoch = 9500\tGradient mag. = 0.008954\n",
      "\tLearning epoch = 9750\tGradient mag. = 0.008726\n",
      "Final gradient magnitude for rate=0.01, mu=1e-06: 0.009\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model = NaiveBayes()\n",
    "gen_model.train(L_train, n_iter=10000, rate=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers below give an estimate of the weights learned for each feature. As you can see, several features have been substantially reweighted (while some others have remained mostly unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.1160965 ,  0.82758228, -0.38315466,  0.97169885,  1.09790897,\n",
       "        9.93726566,  0.98503744,  9.99651539,  1.8207841 ,  9.99909417])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at results on the test set (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust the above classifier (e.g. the labelling functions, the hyper-parameters) and assess the results on the dev/test set. This section shows how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will label a small number of dev/test candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_labeled = 300 # number of candidates to label\n",
    "\n",
    "random_idx = np.random.choice(len(devtest_c), size=(n_labeled,), replace=False)\n",
    "labeled_c = CandidateSet(name='Phenotype Labeled Candidates')\n",
    "for i in random_idx:\n",
    "    labeled_c.append(devtest_c[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use the Snorkel viewer to label a set of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"jupyter-js-widgets\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.cids   = this.model.get('cids');\n",
       "            this.nPages = this.cids.length;\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Initialize all labels from previous sessions\n",
       "            this.labels = this.deserializeDict(this.model.get('_labels_serialized'));\n",
       "            for (var i=0; i < this.nPages; i++) {\n",
       "                this.pid = i;\n",
       "                for (var j=0; j < this.cids[i].length; j++) {\n",
       "                    this.cxid = j;\n",
       "                    for (var k=0; k < this.cids[i][j].length; k++) {\n",
       "                        this.cid = k;\n",
       "                        if (this.cids[i][j][k] in this.labels) {\n",
       "                            this.markCurrentCandidate(false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-context\").click(function() {\n",
       "                that.switchContext(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-context\").click(function() {\n",
       "                that.switchContext(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "            this.$el.find(\"#label-true\").click(function() {\n",
       "                that.labelCandidate(true, true);\n",
       "            });\n",
       "            this.$el.find(\"#label-false\").click(function() {\n",
       "                that.labelCandidate(false, true);\n",
       "            });\n",
       "\n",
       "            // Arrow key functionality\n",
       "            this.$el.keydown(function(e) {\n",
       "                switch(e.which) {\n",
       "                    case 74: // j\n",
       "                    that.switchCandidate(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 73: // i\n",
       "                    that.switchPage(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 76: // l\n",
       "                    that.switchCandidate(1);\n",
       "                    break;\n",
       "\n",
       "                    case 75: // k\n",
       "                    that.switchPage(1);\n",
       "                    break;\n",
       "\n",
       "                    case 84: // t\n",
       "                    that.labelCandidate(true, true);\n",
       "                    break;\n",
       "\n",
       "                    case 70: // f\n",
       "                    that.labelCandidate(false, true);\n",
       "                    break;\n",
       "                }\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Get candidate selector for currently selected candidate, escaping id properly\n",
       "        getCandidate: function() {\n",
       "            return this.$el.find(\".\"+this.cids[this.pid][this.cxid][this.cid]);\n",
       "        },  \n",
       "\n",
       "        // Color the candidate correctly according to registered label, as well as set highlighting\n",
       "        markCurrentCandidate: function(highlight) {\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var tags = this.$el.find(\".\"+cid);\n",
       "\n",
       "            // Clear color classes\n",
       "            tags.removeClass(\"candidate-h\");\n",
       "            tags.removeClass(\"true-candidate\");\n",
       "            tags.removeClass(\"true-candidate-h\");\n",
       "            tags.removeClass(\"false-candidate\");\n",
       "            tags.removeClass(\"false-candidate-h\");\n",
       "            tags.removeClass(\"highlighted\");\n",
       "\n",
       "            if (highlight) {\n",
       "                if (cid in this.labels) {\n",
       "                    tags.addClass(String(this.labels[cid]) + \"-candidate-h\");\n",
       "                } else {\n",
       "                    tags.addClass(\"candidate-h\");\n",
       "                }\n",
       "            \n",
       "            // If un-highlighting, leave with first non-null coloring\n",
       "            } else {\n",
       "                var that = this;\n",
       "                tags.each(function() {\n",
       "                    var cids = $(this).attr('class').split(/\\s+/).map(function(item) {\n",
       "                        return parseInt(item);\n",
       "                    });\n",
       "                    cids.sort();\n",
       "                    for (var i in cids) {\n",
       "                        if (cids[i] in that.labels) {\n",
       "                            var label = that.labels[cids[i]];\n",
       "                            $(this).addClass(String(label) + \"-candidate\");\n",
       "                            $(this).removeClass(String(!label) + \"-candidate\");\n",
       "                            break;\n",
       "                        }\n",
       "                    }\n",
       "                });\n",
       "            }\n",
       "\n",
       "            // Extra highlighting css\n",
       "            if (highlight) {\n",
       "                tags.addClass(\"highlighted\");\n",
       "            }\n",
       "\n",
       "            // Classes for showing direction of relation\n",
       "            if (highlight) {\n",
       "                this.$el.find(\".\"+cid+\"-0\").addClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").addClass(\"right-candidate\");\n",
       "            } else {\n",
       "                this.$el.find(\".\"+cid+\"-0\").removeClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").removeClass(\"right-candidate\");\n",
       "            }\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var N = this.cids[this.pid].length\n",
       "            var M = this.cids[this.pid][this.cxid].length;\n",
       "            if (N == 0 || M == 0) { return false; }\n",
       "\n",
       "            // Clear highlighting from previous candidate\n",
       "            if (inc != 0) {\n",
       "                this.markCurrentCandidate(false);\n",
       "\n",
       "                // Increment the cid counter\n",
       "\n",
       "                // Move to next context\n",
       "                if (this.cid + inc >= M) {\n",
       "                    while (this.cid + inc >= M) {\n",
       "                        \n",
       "                        // At last context on page, halt\n",
       "                        if (this.cxid == N - 1) {\n",
       "                            this.cid = M - 1;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to next context\n",
       "                        } else {\n",
       "                            inc -= M - this.cid;\n",
       "                            this.cxid += 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = 0;\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                // Move to previous context\n",
       "                } else if (this.cid + inc < 0) {\n",
       "                    while (this.cid + inc < 0) {\n",
       "                        \n",
       "                        // At first context on page, halt\n",
       "                        if (this.cxid == 0) {\n",
       "                            this.cid = 0;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to previous context\n",
       "                        } else {\n",
       "                            inc += this.cid + 1;\n",
       "                            this.cxid -= 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = M - 1;\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                // Move within current context\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.markCurrentCandidate(true);\n",
       "\n",
       "            // Push this new cid to the model\n",
       "            this.model.set('_selected_cid', this.cids[this.pid][this.cxid][this.cid]);\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Switch through contexts\n",
       "        switchContext: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "\n",
       "            // Iterate context on this page\n",
       "            var M = this.cids[this.pid].length;\n",
       "            if (this.cxid + inc < 0) {\n",
       "                this.cxid = 0;\n",
       "            } else if (this.cxid + inc >= M) {\n",
       "                this.cxid = M - 1;\n",
       "            } else {\n",
       "                this.cxid += inc;\n",
       "            }\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.cxid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Label currently-selected candidate\n",
       "        labelCandidate: function(label, highlighted) {\n",
       "            var c    = this.getCandidate();\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var cl   = String(label) + \"-candidate\";\n",
       "            var clh  = String(label) + \"-candidate-h\";\n",
       "            var cln  = String(!label) + \"-candidate\";\n",
       "            var clnh = String(!label) + \"-candidate-h\";\n",
       "\n",
       "            // Toggle label highlighting\n",
       "            if (c.hasClass(cl) || c.hasClass(clh)) {\n",
       "                c.removeClass(cl);\n",
       "                c.removeClass(clh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(\"candidate-h\");\n",
       "                }\n",
       "                this.labels[cid] = null;\n",
       "                this.send({event: 'delete_label', cid: cid});\n",
       "            } else {\n",
       "                c.removeClass(cln);\n",
       "                c.removeClass(clnh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(clh);\n",
       "                } else {\n",
       "                    c.addClass(cl);\n",
       "                }\n",
       "                this.labels[cid] = label;\n",
       "                this.send({event: 'set_label', cid: cid, value: label});\n",
       "            }\n",
       "\n",
       "            // Set the label and pass back to the model\n",
       "            this.model.set('_labels_serialized', this.serializeDict(this.labels));\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Serialization of hash maps, because traitlets Dict doesn't seem to work...\n",
       "        serializeDict: function(d) {\n",
       "            var s = [];\n",
       "            for (var key in d) {\n",
       "                s.push(key+\"~~\"+d[key]);\n",
       "            }\n",
       "            return s.join();\n",
       "        },\n",
       "\n",
       "        // Deserialization of hash maps\n",
       "        deserializeDict: function(s) {\n",
       "            var d = {};\n",
       "            var entries = s.split(/,/);\n",
       "            var kv;\n",
       "            for (var i in entries) {\n",
       "                kv = entries[i].split(/~~/);\n",
       "                if (kv[1] == \"true\") {\n",
       "                    d[kv[0]] = true;\n",
       "                } else if (kv[1] == \"false\") {\n",
       "                    d[kv[0]] = false;\n",
       "                }\n",
       "            }\n",
       "            return d;\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "sv = SentenceNgramViewer(labeled_c, session, annotator_name=\"Snorkel Phenotype Annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display the viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare these labels against the predictions of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 150 ms, sys: 29.2 ms, total: 179 ms\n",
      "Wall time: 318 ms\n",
      "Generating annotations for 300 candidates...\n",
      "[========================================] 100%\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 6.15 s, sys: 298 ms, total: 6.45 s\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "\n",
    "label_manager = LabelManager()\n",
    "\n",
    "%time Y_test = label_manager.load(session, labeled_c, 'Snorkel Phenotype Annotations')\n",
    "%time L_test = label_manager.create(session, labeled_c, 'Phenotype LF Test Labels', f=LFs)\n",
    "# session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_model.score(L_test, Y_test, labeled_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify all the papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a classifier that can score phenotype mentions in the text. Let's apply this classifier to assign a phenotype to each of our papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze / Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If a mention occurs in the title, its probably correct, we can take it.\n",
    "\n",
    "Question: what papers did not have any disease mentions in the title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating annotations for 7468 candidates...\n",
      "[========================================] 100%\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 1min 34s, sys: 534 ms, total: 1min 35s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "\n",
    "label_manager = LabelManager()\n",
    "\n",
    "# delete existing labels\n",
    "session.rollback()\n",
    "session.query(AnnotationKeySet).filter(AnnotationKeySet.name == 'Phenotype LF All Labels').delete()\n",
    "%time L_all = label_manager.create(session, phen_c, 'Phenotype LF All Labels', f=LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize what we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = gen_model.odds(L_all)\n",
    "score_dict = { doc.name : list() for doc in corpus.documents }\n",
    "for s, c in zip(scores, phen_c):\n",
    "    score_dict[c[0].parent.document.name].append((s,c))\n",
    "\n",
    "results = dict()\n",
    "for pmid, preds in score_dict.items():\n",
    "    if preds: \n",
    "        best_c = sorted(preds, reverse=True)[0][1]\n",
    "        results[best_c[0].parent.document.name] = best_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 23401653 A genome-wide association study for corneal curvature identifies the platelet-derived growth factor receptor &#x003b1; gene as a quantitative trait locus for eye size in white Europeans.\n",
      "SnorkelPhenotype(Span(\"a quantitative trait\", parent=5231, chars=[123,142], words=[16,18])) [15, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[(32.713146397913356, SnorkelPhenotype(Span(\"a quantitative trait\", parent=5231, chars=[123,142], words=[16,18]))), (30.92358318810064, SnorkelPhenotype(Span(\"corneal\", parent=5231, chars=[36,42], words=[5,5]))), (30.797373076203804, SnorkelPhenotype(Span(\"platelet-derived growth factor\", parent=5231, chars=[69,98], words=[9,11]))), (29.825674221961485, SnorkelPhenotype(Span(\"eye size\", parent=5231, chars=[154,161], words=[21,22]))), (29.825674221961485, SnorkelPhenotype(Span(\"size in\", parent=5231, chars=[158,164], words=[22,23])))]\n",
      "\n",
      "Document 22423221 A meta-analysis and genome-wide association study of platelet count and mean platelet volume in african americans.\n",
      "SnorkelPhenotype(Span(\"Mean Platelet Volume\", parent=4056, chars=[72,91], words=[10,12])) [15, 0, 5, 1, 0, 0, 0, 0, 0, 0]\n",
      "[(30.797373076203804, SnorkelPhenotype(Span(\"Mean Platelet Volume\", parent=4056, chars=[72,91], words=[10,12]))), (29.825674221961485, SnorkelPhenotype(Span(\"Platelet Count\", parent=4056, chars=[53,66], words=[7,8]))), (1.0979089661391537, SnorkelPhenotype(Span(\"platelet count\", parent=4057, chars=[41,54], words=[5,6]))), (0.97169885424231905, SnorkelPhenotype(Span(\"mean platelet volume\", parent=4057, chars=[60,79], words=[8,10]))), (-1.8207841004818972, SnorkelPhenotype(Span(\"platelet count\", parent=4058, chars=[158,171], words=[25,26])))]\n",
      "\n",
      "Document 17903292 A genome-wide association for kidney function and endocrine-related traits in the NHLBI's Framingham Heart Study.\n",
      "SnorkelPhenotype(Span(\"endocrine-related traits\", parent=640, chars=[50,73], words=[7,8])) [15, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(31.741447543671036, SnorkelPhenotype(Span(\"endocrine-related traits\", parent=640, chars=[50,73], words=[7,8]))), (30.92358318810064, SnorkelPhenotype(Span(\"kidney\", parent=640, chars=[30,35], words=[4,4]))), (2.0696078203814725, SnorkelPhenotype(Span(\"Glomerular filtration rate\", parent=641, chars=[0,25], words=[0,2]))), (1.7992811302869307, SnorkelPhenotype(Span(\"urinary albumin excretion\", parent=641, chars=[37,61], words=[7,9]))), (0.82758227604461165, SnorkelPhenotype(Span(\"of kidney\", parent=641, chars=[81,89], words=[15,16])))]\n",
      "\n",
      "Document 23532257 Genome-wide association study in a Chinese population identifies a susceptibility locus for type 2 diabetes at 7q32 near PAX4.\n",
      "SnorkelPhenotype(Span(\"type 2 diabetes\", parent=5394, chars=[92,106], words=[12,14])) [15, 0, 5, 1, 0, 0, 0, 0, 0, 0]\n",
      "[(30.797373076203804, SnorkelPhenotype(Span(\"type 2 diabetes\", parent=5394, chars=[92,106], words=[12,14]))), (2.0696078203814725, SnorkelPhenotype(Span(\"type 2 diabetes\", parent=5395, chars=[37,51], words=[5,7]))), (-0.84908524623957815, SnorkelPhenotype(Span(\"type 2 diabetes\", parent=5396, chars=[123,137], words=[20,22]))), (-499.95470848020221, SnorkelPhenotype(Span(\"populations\", parent=5395, chars=[86,96], words=[13,13]))), (-501.7754925806841, SnorkelPhenotype(Span(\"population\", parent=5396, chars=[65,74], words=[11,11])))]\n",
      "\n",
      "Document 20663923 A genome-wide scan for common alleles affecting risk for autism.\n",
      "SnorkelPhenotype(Span(\"autism\", parent=2141, chars=[57,62], words=[9,9])) [15, 0, 5, 0, 0, 0, 0, 0, 0, 0]\n",
      "[(29.825674221961485, SnorkelPhenotype(Span(\"autism\", parent=2141, chars=[57,62], words=[9,9]))), (2.0696078203814725, SnorkelPhenotype(Span(\"autism spectrum disorders\", parent=2142, chars=[9,33], words=[1,3]))), (-1.8207841004818972, SnorkelPhenotype(Span(\"Autism\", parent=2143, chars=[39,44], words=[7,7]))), (-101.78593800949686, SnorkelPhenotype(Span(\"rigorously\", parent=2143, chars=[93,102], words=[16,16]))), (-501.7754925806841, SnorkelPhenotype(Span(\"association with\", parent=2143, chars=[215,230], words=[34,35])))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in corpus.documents[:5]:\n",
    "    print d, kb.paper_by_pmid(d.name).title\n",
    "    print unicode(results.get(d.name, None)), [LF(results.get(d.name)) for LF in LFs]\n",
    "    try:\n",
    "        print sorted(score_dict[d.name], reverse=True)[:5]\n",
    "    except UnicodeEncodeError:\n",
    "        print 'Unicode error'\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "nltk_stopword_set = set(nltk_stopwords.words('english'))\n",
    "\n",
    "def clean_stopwords(txt):\n",
    "    words = txt.split()\n",
    "    i = 0\n",
    "    new_words = []\n",
    "    while i < len(words):\n",
    "        i += 1\n",
    "        if words[i-1] in nltk_stopword_set or words[i-1] in punctuation: continue\n",
    "        new_word = words[i-1].strip(',.;')\n",
    "        if new_word.islower() or new_word.isupper(): new_word = new_word.title()\n",
    "        new_words.append(new_word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "with open('phenotypes.extracted.latest.tsv', 'w') as f:\n",
    "    for d in corpus.documents:\n",
    "        # pick the top two results:\n",
    "        best = sorted(score_dict[d.name], reverse=True)[:3]\n",
    "        # if both are in title, report both, otherwise report only the best one\n",
    "        if len(best) == 3 and best[2][1][0].parent.position == 0 and best[1][0] - best[2][0] < 3:\n",
    "            (_, r1), (_, r2), (_, r3) = best\n",
    "            phen = '|'.join(set([clean_stopwords(r[0].get_span()) for s,r in best[:3]]))\n",
    "        elif len(best) >= 2 and best[1][1][0].parent.position == 0 and best[1][0] > 5:\n",
    "            phen = '|'.join(set([clean_stopwords(r[0].get_span()) for s,r in best[:2]]))                \n",
    "        else:\n",
    "            phen = clean_stopwords(best[0][1][0].get_span())\n",
    "        out_str = u'%s\\t%s\\t\\n' % (d.name, phen)        \n",
    "        f.write(out_str.encode(\"UTF-8\"))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.2"
  },
  "widgets": {
   "state": {
    "723274901c034be8b4ea1acf29f5eddc": {
     "views": [
      {
       "cell_index": 63
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
