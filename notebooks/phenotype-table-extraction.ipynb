{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenotype/SNP relation extraction from tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will demo the module that parses tables in papers and extracts relations between SNPs and phenotypes (in cases in which the paper discusses multiple phenotypes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by configuring Jupyter and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "\n",
    "# set the paths to snorkel and gwasdb\n",
    "sys.path.append('../snorkel-tables')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/crawler')\n",
    "\n",
    "# set up the directory with the input papers\n",
    "abstract_dir = '../data/db/papers'\n",
    "\n",
    "# set up matplotlib\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "\n",
    "# create a Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our usual corpus of GWAS papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from extractor.parser import UnicodeXMLTableDocParser\n",
    "from snorkel.parser import XMLMultiDocParser\n",
    "\n",
    "xml_parser = XMLMultiDocParser(\n",
    "    path=abstract_dir,\n",
    "    doc='./*',\n",
    "    text='.//table',\n",
    "    id='.//article-id[@pub-id-type=\"pmid\"]/text()',\n",
    "    keep_xml_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus of 589 documents\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser, OmniParser\n",
    "from snorkel.models import Corpus\n",
    "\n",
    "# parses tables into rows, cols, cells...\n",
    "table_parser = OmniParser(timeout=1000000)\n",
    "\n",
    "try:\n",
    "    corpus = session.query(Corpus).filter(Corpus.name == 'GWAS Table Corpus').one()\n",
    "except:\n",
    "    cp = CorpusParser(xml_parser, table_parser)\n",
    "    %time corpus = cp.parse_corpus(name='GWAS Table Corpus', session=session)\n",
    "    session.add(corpus)\n",
    "    session.commit()\n",
    "\n",
    "print 'Loaded corpus of %d documents' % len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define candidate matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RSid matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import RegexMatchSpan\n",
    "rsid_matcher = RegexMatchSpan(rgx=r'rs\\d+(/[ATCG]{1,2})*$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phenotype matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first matcher checks if we are in a column whose header labels it as a phenotype column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import CellNameDictionaryMatcher\n",
    "\n",
    "phen_words = ['trait', 'phenotype', 'outcome'] # words that denote phenotypes\n",
    "phen_matcher = CellNameDictionaryMatcher(axis='col', d=phen_words, n_max=3, ignore_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next matcher will match phenotypes in cells that span an entire axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch\n",
    "from db.kb import KnowledgeBase\n",
    "from extractor.util import make_ngrams\n",
    "\n",
    "# collect phenotype list\n",
    "kb = KnowledgeBase()\n",
    "# efo phenotypes\n",
    "efo_phenotype_list0 = kb.get_phenotype_candidates(source='efo', peek=True) # TODO: remove peaking\n",
    "efo_phenotype_list = list(make_ngrams(efo_phenotype_list0))\n",
    "# mesh diseases\n",
    "mesh_phenotype_list0 = kb.get_phenotype_candidates(source='mesh')\n",
    "mesh_phenotype_list = list(make_ngrams(mesh_phenotype_list0))\n",
    "# mesh chemicals\n",
    "chem_phenotype_list = kb.get_phenotype_candidates(source='chemical')\n",
    "\n",
    "phenotype_names = efo_phenotype_list + mesh_phenotype_list + chem_phenotype_list\n",
    "phen_name_matcher = DictionaryMatch(d=phenotype_names, ignore_case=True, stemmer='porter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Relation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "from snorkel.throttlers import AlignmentThrottler, SeparatingSpanThrottler, OrderingThrottler, CombinedThrottler\n",
    "\n",
    "# create a Snorkel class for the relation we will extract\n",
    "from snorkel.models import candidate_subclass\n",
    "RsidPhenRel = candidate_subclass('RsidPhenRel', ['rsid','phen'])\n",
    "\n",
    "# define our candidate spaces\n",
    "from snorkel.candidates import TableNgrams, TableCells, SpanningTableCells\n",
    "unigrams = TableNgrams(n_max=1)\n",
    "cells = TableCells()\n",
    "spanning_cells = SpanningTableCells(axis='row')\n",
    "\n",
    "# we will be looking only at aligned cells\n",
    "row_align_filter = AlignmentThrottler(axis='row', infer=True)\n",
    "\n",
    "# and at cells where the phenotype is in a spanning header cell above the rsid cell\n",
    "sep_span_filter = SeparatingSpanThrottler(align_axis='col') # rsid and phen are not separated by spanning cells\n",
    "col_order_filter = OrderingThrottler(axis='col', first=1) # phen spanning cell comes first\n",
    "header_filter = CombinedThrottler([sep_span_filter, col_order_filter]) # combine the two throttlers\n",
    "\n",
    "# the first extractor looks at phenotype names in columns with a header indicating it's a phenotype\n",
    "ce1 = CandidateExtractor(RsidPhenRel, [unigrams, cells], [rsid_matcher, phen_matcher], throttler=row_align_filter)\n",
    "\n",
    "# the second extractor looks at phenotype names in columns with a header indicating it's a phenotype\n",
    "ce2 = CandidateExtractor(RsidPhenRel, [unigrams, spanning_cells], [rsid_matcher, phen_name_matcher], throttler=header_filter, stop_on_duplicates=False)\n",
    "\n",
    "# collect that cells that will be searched for candidates\n",
    "tables = [table for doc in corpus.documents for table in doc.tables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform relation extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    rels1 = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Set 1').one()\n",
    "except:\n",
    "    %time rels1 = ce1.extract(tables, 'RsidPhenRel Set 1', session)\n",
    "    \n",
    "print \"%s relations extracted, e.g.\" % len(rels1)\n",
    "for cand in rels1[:10]:\n",
    "    print cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    rels2 = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Set 2').one()\n",
    "except:\n",
    "    %time rels2 = ce2.extract(tables, 'RsidPhenRel Set 2', session)\n",
    "    \n",
    "print \"%s relations extracted, e.g.\" % len(rels2)\n",
    "for cand in rels2[:10]: \n",
    "    print cand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the two sets of candiates into a single set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3504 candidates in total\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    rels = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Canidates').one()\n",
    "except:\n",
    "    rels = CandidateSet(name='RsidPhenRel Canidates')\n",
    "    for c in rels1: rels.append(c)\n",
    "    for c in rels2: rels.append(c)\n",
    "\n",
    "    session.add(rels)\n",
    "    session.commit()\n",
    "\n",
    "print '%d candidates in total' % len(rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning the correctness of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train machine learning models to identify which phenotype candidates are actually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a labeled set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split data into an (unlabeled) training set (since we will use unsupervised risk estimation to train a candidate on it), and a dev/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 1752 training and 1752 dev/testing candidates\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_c = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Training Candidates').one()\n",
    "    devtest_c = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Dev/Test Candidates').one()\n",
    "except:\n",
    "    # delete any previous sets with that name\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Training Candidates').delete()\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Dev/Test Candidates').delete()\n",
    "\n",
    "    frac_test = 0.5\n",
    "\n",
    "    # initialize the new sets\n",
    "    train_c = CandidateSet(name='RsidPhenRel Training Candidates')\n",
    "    devtest_c = CandidateSet(name='RsidPhenRel Dev/Test Candidates')\n",
    "\n",
    "    # choose a random subset for the labeled set\n",
    "    n_test = len(rels) * frac_test\n",
    "    test_idx = set(np.random.choice(len(rels), size=(n_test,), replace=False))\n",
    "\n",
    "    # add to the sets\n",
    "    for i, c in enumerate(rels):\n",
    "        if i in test_idx:\n",
    "            devtest_c.append(c)\n",
    "        else:\n",
    "            train_c.append(c)\n",
    "\n",
    "    # save the results\n",
    "    session.add(train_c)\n",
    "    session.add(devtest_c)\n",
    "    session.commit()\n",
    "\n",
    "print 'Initialized %d training and %d dev/testing candidates' % (len(train_c), len(devtest_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Labelling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the data programming approach, we define set of labeling functions. We will learn their accuracy via unsupervised learning and use them for classifying candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 162 of the file /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "s=None\n",
    "doc = [d for d in corpus.documents if d.name == '17903303'][0]\n",
    "table = doc.tables[3]\n",
    "for cell in table.cells:\n",
    "    top_cells = get_aligned_cells(cell, 'col', infer=True)\n",
    "    top_phrases = [phrase for cell in top_cells for phrase in cell.phrases]\n",
    "# rels[0][1].parent.table.cells[0].phrases\n",
    "# corpus.documents[0].phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "\n",
    "bad_words = ['rs number', 'rs id', 'rsid']\n",
    "\n",
    "# negative LFs\n",
    "def LF_number(m):\n",
    "    txt = m[1].get_span()\n",
    "    frac_num = len([ch for ch in txt if ch.isdigit()]) / float(len(txt))\n",
    "    return -1 if len(txt) > 5 and frac_num > 0.4 or frac_num > 0.6 else 0\n",
    "\n",
    "def LF_bad_phen_mentions(m):\n",
    "    if cell_spans(m[1].parent.cell, m[1].parent.table, 'row'): return 0\n",
    "    #     if m[1].context.cell.spans('row'): return 0\n",
    "    top_cells = get_aligned_cells(m[1].parent.cell, 'col', infer=True)\n",
    "    top_cells = [cell for cell in top_cells]\n",
    "#     top_cells = m.span1.context.cell.aligned_cells(axis='col', induced=True)\n",
    "    try:\n",
    "        top_phrases = [phrase for cell in top_cells for phrase in cell.phrases]\n",
    "    except:\n",
    "        for cell in top_cells:\n",
    "            print cell, cell.phrases\n",
    "    if not top_phrases: return 0\n",
    "    matching_phrases = []\n",
    "    for phrase in top_phrases:\n",
    "        if any (phen_matcher._f_ngram(word) for word in phrase.text.split(' ')):\n",
    "            matching_phrases.append(phrase)\n",
    "    small_matching_phrases = [phrase for phrase in matching_phrases if len(phrase.text) <= 25]\n",
    "    return -1 if not small_matching_phrases else 0\n",
    "\n",
    "def LF_bad_word(m):\n",
    "    txt = m[1].get_span()\n",
    "    return -1 if any(word in txt for word in bad_words) else 0\n",
    "\n",
    "LF_tables_neg = [LF_number, LF_bad_phen_mentions]\n",
    "\n",
    "# positive LFs\n",
    "def LF_no_neg(m):\n",
    "    return +1 if not any(LF(m) for LF in LF_tables_neg) else 0\n",
    "\n",
    "LF_tables_pos = [LF_no_neg]\n",
    "\n",
    "LFs = LF_tables_neg + LF_tables_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate features for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating annotations for 1752 candidates...\n",
      "[========================================] 100%\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 9min 23s, sys: 2min 42s, total: 12min 5s\n",
      "Wall time: 12min 13s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "try:\n",
    "    %time L_train = label_manager.load(session, train_c, 'RsidPhenRel LF Labels6')\n",
    "except sqlalchemy.orm.exc.NoResultFound:\n",
    "    %time L_train = label_manager.create(session, train_c, 'RsidPhenRel LF Labels6', f=LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at some basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conflicts</th>\n",
       "      <th>coverage</th>\n",
       "      <th>j</th>\n",
       "      <th>overlaps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_number</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0.144242</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.072976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_bad_phen_mentions</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0.152223</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0.072976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_no_neg</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0.776511</td>\n",
       "      <td> 2</td>\n",
       "      <td> 0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      conflicts  coverage  j  overlaps\n",
       "LF_number                     0  0.144242  0  0.072976\n",
       "LF_bad_phen_mentions          0  0.152223  1  0.072976\n",
       "LF_no_neg                     0  0.776511  2  0.000000\n",
       "\n",
       "[3 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a generative model, just like in the phenotype extraction notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/matplotlib/__init__.py:1155: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t1752\n",
      "Features:\t\t\t3\n",
      "================================================================================\n",
      "Begin training for rate=0.01, mu=1e-06\n",
      "\tLearning epoch = 0\tGradient mag. = 0.084432\n",
      "\tLearning epoch = 250\tGradient mag. = 0.098711\n",
      "\tLearning epoch = 500\tGradient mag. = 0.107868\n",
      "\tLearning epoch = 750\tGradient mag. = 0.116008\n",
      "\tLearning epoch = 1000\tGradient mag. = 0.122948\n",
      "\tLearning epoch = 1250\tGradient mag. = 0.128657\n",
      "\tLearning epoch = 1500\tGradient mag. = 0.133226\n",
      "\tLearning epoch = 1750\tGradient mag. = 0.136810\n",
      "\tLearning epoch = 2000\tGradient mag. = 0.139583\n",
      "\tLearning epoch = 2250\tGradient mag. = 0.141712\n",
      "\tLearning epoch = 2500\tGradient mag. = 0.143340\n",
      "\tLearning epoch = 2750\tGradient mag. = 0.144582\n",
      "\tLearning epoch = 3000\tGradient mag. = 0.145530\n",
      "\tLearning epoch = 3250\tGradient mag. = 0.146254\n",
      "\tLearning epoch = 3500\tGradient mag. = 0.146807\n",
      "\tLearning epoch = 3750\tGradient mag. = 0.147231\n",
      "\tLearning epoch = 4000\tGradient mag. = 0.147556\n",
      "\tLearning epoch = 4250\tGradient mag. = 0.147805\n",
      "\tLearning epoch = 4500\tGradient mag. = 0.147996\n",
      "\tLearning epoch = 4750\tGradient mag. = 0.148143\n",
      "\tLearning epoch = 5000\tGradient mag. = 0.148256\n",
      "\tLearning epoch = 5250\tGradient mag. = 0.148343\n",
      "\tLearning epoch = 5500\tGradient mag. = 0.148410\n",
      "\tLearning epoch = 5750\tGradient mag. = 0.148462\n",
      "\tLearning epoch = 6000\tGradient mag. = 0.148501\n",
      "\tLearning epoch = 6250\tGradient mag. = 0.148532\n",
      "\tLearning epoch = 6500\tGradient mag. = 0.148555\n",
      "\tLearning epoch = 6750\tGradient mag. = 0.148574\n",
      "\tLearning epoch = 7000\tGradient mag. = 0.148588\n",
      "\tLearning epoch = 7250\tGradient mag. = 0.148598\n",
      "\tLearning epoch = 7500\tGradient mag. = 0.148607\n",
      "\tLearning epoch = 7750\tGradient mag. = 0.148613\n",
      "\tLearning epoch = 8000\tGradient mag. = 0.148618\n",
      "\tLearning epoch = 8250\tGradient mag. = 0.135731\n",
      "\tLearning epoch = 8500\tGradient mag. = 0.120952\n",
      "\tLearning epoch = 8750\tGradient mag. = 0.087698\n",
      "\tLearning epoch = 9000\tGradient mag. = 0.061846\n",
      "\tLearning epoch = 9250\tGradient mag. = 0.043708\n",
      "\tLearning epoch = 9500\tGradient mag. = 0.030977\n",
      "\tLearning epoch = 9750\tGradient mag. = 0.022038\n",
      "Final gradient magnitude for rate=0.01, mu=1e-06: 0.016\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model = NaiveBayes()\n",
    "gen_model.train(L_train, n_iter=10000, rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.92867574,  9.91268375,  0.98503744])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify all the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating annotations for 3504 candidates...\n",
      "[========================================] 100%\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 13min 15s, sys: 3min 22s, total: 16min 37s\n",
      "Wall time: 16min 47s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "%time L_all = label_manager.create(session, rels, 'RsidPhenRel LF All Labels', f=LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2745 relations extracted, e.g.:\n",
      "[(u'17903300', u'rs7202384', u'Mean BMI'), (u'17903300', u'rs10486301', u'Mean BMI'), (u'17903300', u'rs7533902', u'Mean BMI'), (u'17903300', u'rs2226351', u'Mean WC'), (u'17903300', u'rs711702', u'Mean BMI'), (u'17903300', u'rs2296465', u'Mean BMI'), (u'17903300', u'rs10517461', u'Mean WC'), (u'17903300', u'rs1875517', u'Mean WC'), (u'17903300', u'rs4312989', u'Mean WC'), (u'17903300', u'rs2361128', u'Mean BMI')]\n"
     ]
    }
   ],
   "source": [
    "preds = gen_model.odds(L_all)\n",
    "good_rels = [(c[0].parent.document.name, c[0].get_span(), c[1].get_span()) for (c, p) in zip(rels, preds) if p > 0]\n",
    "print len(good_rels), 'relations extracted, e.g.:'\n",
    "print good_rels[:10]\n",
    "\n",
    "# store relations to annotate\n",
    "with open('rels.acronyms.extracted.tsv', 'w') as f:\n",
    "    for doc_id, str1, str2 in good_rels:\n",
    "        try:\n",
    "            out = u'{}\\t{}\\t{}\\n'.format(doc_id, unicode(str1), str2)\n",
    "            f.write(out.encode(\"UTF-8\"))\n",
    "        except:\n",
    "            print 'Error in saving:', str1, str2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve acronyms based on ones extracted earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326 definitions loaded\n"
     ]
    }
   ],
   "source": [
    "from extractor.dictionary import Dictionary, unravel\n",
    "\n",
    "D = Dictionary()\n",
    "D.load('acronyms.extracted.all.tsv')\n",
    "print len(D), 'definitions loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dictionary to resolve acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rels = [ (doc_id, rs_id, unravel(doc_id, phen, D)) for doc_id, rs_id, phen in rels ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combine with extracted pvalue/rsid relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pval_rsid_dict = dict()\n",
    "pval_dict = dict() # combine all of the pvalues for a SNPs in the same document into one set\n",
    "with open('pval-rsid.raw.tsv') as f:\n",
    "    for line in f:\n",
    "        pmid, rsid, table_id, row_id, col_id, pval = line.strip().split('\\t')\n",
    "        pval, table_id, row_id, col_id = float(pval), int(table_id), int(row_id), int(col_id)\n",
    "        \n",
    "        if pmid not in pval_rsid_dict: pval_rsid_dict[pmid] = dict()\n",
    "        key = (rsid, table_id, row_id)\n",
    "        if key not in pval_rsid_dict[pmid]: pval_rsid_dict[pmid][key] = set()\n",
    "        pval_rsid_dict[pmid][key].add(pval)\n",
    "                \n",
    "        if pmid not in pval_dict: pval_dict[pmid] = dict()\n",
    "        if rsid not in pval_dict[pmid]: pval_dict[pmid][rsid] = set()\n",
    "        pval_dict[pmid][rsid].add(pval)\n",
    "\n",
    "pval_dict0 = {pmid : {rsid : min(pval_dict[pmid][rsid]) for rsid in pval_dict[pmid]} for pmid in pval_dict}\n",
    "pval_rsid_dict0 = {pmid : {key : min(pval_rsid_dict[pmid][key]) for key in pval_rsid_dict[pmid]} for pmid in pval_rsid_dict}\n",
    "pval_dict = pval_dict0\n",
    "pval_rsid_dict = pval_rsid_dict0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan. If phen/rsid has been extracted from tables: take its pvalue from pval_rsid_dict.\n",
    "\n",
    "If not, we assume that paper has only one phenotype and we take the smallest reported pvalue in the paper.\n",
    "\n",
    "Our goal for now is just to filter phen/rsid relations that have pval<1e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save all relations that are sufficiently small p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preds = learner.predict_wmv(candidates)\n",
    "predicted_candidates = [c for (c, p) in zip(candidates, preds) if p == 1]\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "def _normalize_str(s):\n",
    "    try:\n",
    "        s = s.encode('utf-8')\n",
    "        return s\n",
    "    except UnicodeEncodeError: \n",
    "        pass\n",
    "    try:\n",
    "        s = s.decode('utf-8')\n",
    "        return s\n",
    "    except UnicodeDecodeError: \n",
    "        pass    \n",
    "    raise Exception()\n",
    "    \n",
    "def clean_rsid(rsid):\n",
    "    return re.sub('/.+', '', rsid)\n",
    "\n",
    "with open('phen-rsid.table.rel.all.tsv', 'w') as f:\n",
    "    for c in predicted_candidates:\n",
    "        pmid = c.span0.context.document.name\n",
    "        rsid = c.span0.get_span()\n",
    "        phen = c.span1.get_span()        \n",
    "        table_id = c.span0.context.table.position\n",
    "        row_num = c.span0.context.cell.row_num\n",
    "        col_num = c.span0.context.cell.col_num # of the rsid\n",
    "\n",
    "        phen = (unravel(pmid, phen, D))\n",
    "        if isinstance(phen, unicode):\n",
    "            phen = phen.encode('utf-8')\n",
    "        \n",
    "        try:\n",
    "            pval = pval_rsid_dict[pmid].get((rsid, table_id, row_num), -1)\n",
    "        except KeyError:\n",
    "            pval = -1\n",
    "#             continue\n",
    "        if pval > 1e-5: continue\n",
    "\n",
    "        out_str = '{pmid}\\t{rsid}\\t{phen}\\t{pval}\\ttable\\t{table_id}\\t{row}\\t{col}\\n'.format(\n",
    "                    pmid=pmid, rsid=clean_rsid(rsid), phen=phen, pval=pval, table_id=table_id, row=row_num, col=col_num)\n",
    "        f.write(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print [(c, c.span0.context.cell.row_num, unravel(c.span0.context.document.name, c.span1.get_span(), D)) for c in candidates if c.span0.get_span() == 'rs10500631']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
